{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d0b4b8",
   "metadata": {},
   "source": [
    "# Reinforcement Leaning for Connect 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4318326",
   "metadata": {},
   "source": [
    "**Summary** \n",
    "In this project, we use two reinforcement learning approaches: Q learning and Minimax to develop two agents that can play connect4\n",
    "\n",
    "Khushi- \n",
    "Mohamed-\n",
    "Sajal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d599f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.1 (SDL 2.0.14, Python 3.6.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import pygame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee086c3e",
   "metadata": {},
   "source": [
    "### Game Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with some generic  varaibles and fuctions\n",
    "# since we want 2 agents to play, we need different colorus\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (255, 0, 0)\n",
    "     \n",
    "height =400\n",
    "width =640\n",
    "class display(object): #  class constructs the game representation\n",
    "    def __init__(self,height, width):\n",
    "        pygame.init()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        \n",
    "class board(): #class constructs the game board and how to interact with it\n",
    "    def __init__(self, num_rows, num_columns):\n",
    "        self.rows = rows_num\n",
    "        self.columns = columns_num\n",
    "        self.total_slots = rows_num * columns_num\n",
    "        self.filled_slots = 0    \n",
    "    def prev_state(self): # return presious \n",
    "        for x in self.prev_state:\n",
    "            previous = tuple(x) \n",
    "        return previous\n",
    "    \n",
    "win_seq = 4\n",
    "def coin_type(self): # return coin type\n",
    "        return self.coin_type\n",
    "    \n",
    "class game_logic: # construct the game logic\n",
    "    \n",
    "    def __init__(self, board):\n",
    "        self.board = board\n",
    "        self.board_rows = rows_num\n",
    "        self.board_cols = columns_num\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a6c53",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65dff3",
   "metadata": {},
   "source": [
    "###  Q learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77131fd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-8f3c3ba74b6c>, line 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-8f3c3ba74b6c>\"\u001b[1;36m, line \u001b[1;32m61\u001b[0m\n\u001b[1;33m    self.q[(board.get_prev_state(), chosen_action)] = previous + self.alpha * ((prize + (self.gamma*max_Qnew)) - previous)\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Start epsilon with on and decrease it\n",
    "\n",
    "epsilon = 0.2 # For epsilon Greedy Strategy to determine exploration\n",
    "alpha = 0.3   # learning rate\n",
    "gamma = 0.9   # Discount factor\n",
    "\n",
    "class QLearning_Agent(Agent): # class implementing the Q learning Approch \n",
    "    \n",
    "    def __init__(self, coin_type, epsilon, alpha, gamma): # start the Q Learning Agent and identify its parameters\n",
    "        \n",
    "        Agent.__init__(self, coin_type)\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon \n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        \n",
    "    def get_QValue(self, state, action): # return the propability of a (state, action) pair for next action\n",
    "        \n",
    "        # encourage exploration; \"optimistic\" 1.0 initial values\n",
    "        if self.q.get((state, action)) is None:\n",
    "            self.q[(state, action)] = 1.0\n",
    "        return self.q.get((state, action))    \n",
    "        \n",
    "    def choose_action(self, state, actions): # return best action based on Q-table\n",
    "        current_state = state\n",
    "\n",
    "        if random.random() < self.epsilon: # prefer exploration over exploitation\n",
    "            chosen_action = random.choice(actions)\n",
    "            return chosen_action\n",
    "         \n",
    "        for a in actions: #scan the Q-table for all the movment options\n",
    "             Q_state = [self.get_QValue(current_state, a)]\n",
    "        maxQ = max(Q_state)    # returns the highest reward action\n",
    "        \n",
    "        multiples =[]\n",
    "        if Q_state.count(maxQ) > 1: # choose a random highest option incase of multplies\n",
    "            for i in range(len(actions)):\n",
    "                if Q_state[i] == maxQ:\n",
    "                    multiples.append(i)\n",
    "                    i = random.choice(multiples)\n",
    "                    continue \n",
    "\n",
    "        return actions[i]\n",
    "        \n",
    "    def learn(self, board, actions, chosen_action, game_over, game_logic): # Update the Q-table with newly determined prize\n",
    "        \n",
    "        reward = 0 # start with reward 0 and then update it based on win_value\n",
    "        if (game_over):\n",
    "            win_value = game_logic.get_winner()\n",
    "            if win_value == 0:\n",
    "                prize = 0.5\n",
    "            elif win_value == self.coin_type:\n",
    "                prize = 1\n",
    "            else:\n",
    "                prize = -2\n",
    "                \n",
    "        \n",
    "        previous = self.get_QValue(board.prev_state(), chosen_action)\n",
    "        for a in actions:\n",
    "            max_Qnew = max([self.get_QValue(board.get_state(), a)]\n",
    "        self.q[(boardprev_state(), chosen_action)] = previous + self.alpha * ((prize + (self.gamma*max_Qnew)) - previous) \n",
    "                          \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee87ea6",
   "metadata": {},
   "source": [
    "### MiniMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810faf7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-24-839cf28f15a4>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-839cf28f15a4>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    return(None, 100000000000000)\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def is_terminal_node(board):\n",
    "               return w_m(board, PLAYER_PIECE) or w_m(board, AI_PIECE) or len(positions(board)) == 0\n",
    "def minimax(board,position, depth, alpha, beta, maximizingPlayer):\n",
    "    is_terminal = is_terminal_node(board) #w_m = winning_move   \n",
    "if depth == 0 or is_terminal:\n",
    "    if is_terminal:\n",
    "        if w_m(board, AI_PIECE):\n",
    "        return(None, 100000000000000)\n",
    "        elif w_m(board, PLAYER_PIECE):\n",
    "            return(None, -10000000000000)\n",
    "        else: # return static evaluation of position(game over)\n",
    "            return (None, 0)\n",
    "else: # d==0\n",
    "    return (None, s_p(board, AI_PIECE))\n",
    "    #s_p= score_position\n",
    "if maximizingPlayer:\n",
    "    maxeval = -infinity\n",
    "    column = random.choice(positions)\n",
    "    for col in positions:\n",
    "        row = get_next_open_row(board, col)\n",
    "        b_copy = board.copy()\n",
    "        drop_piece(b_copy, row, col, AI_PIECE)\n",
    "        eval = minimax(b_copy, depth-1, False)[1]\n",
    "        if eval > value:\n",
    "            value = maxeval\n",
    "            column = col\n",
    "            maxeval = max(maxeval, eval)\n",
    "alpha =max(alpha,eval)\n",
    "if beta<= alpha:\n",
    "               break\n",
    "               return maxeval\n",
    "else: # Minimizing player\n",
    "        mineval = infinity\n",
    "        column = random.choice(positions)\n",
    "        for col in positions:\n",
    "            row = get_next_open_row(board, col)\n",
    "            b_copy = board.copy()\n",
    "            drop_piece(b_copy, row, col, PLAYER_PIECE)\n",
    "            eval = minimax(b_copy, depth-1, True)[1]\n",
    "            if eval < value:\n",
    "                value = mineval\n",
    "                column = col\n",
    "mineval=min(mineval,eval)\n",
    "beta= min(beta,eval)\n",
    "if beta<= alpha:\n",
    "                break\n",
    "return mineval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b6f05",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3769dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac78e13",
   "metadata": {},
   "source": [
    "### The 2 Agents Play Against each Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765bb690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
